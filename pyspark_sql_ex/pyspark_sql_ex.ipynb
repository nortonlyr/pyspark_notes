{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f630051",
   "metadata": {},
   "source": [
    "reference https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b4da3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from source\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import csv\n",
    "# import random\n",
    "# import string\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# random.seed(1999)\n",
    "\n",
    "# letters = string.ascii_lowercase\n",
    "# letters_upper = string.ascii_uppercase\n",
    "# for _i in range(0, 10):\n",
    "#     letters += letters\n",
    "\n",
    "# for _i in range(0, 10):\n",
    "#     letters += letters_upper\n",
    "\n",
    "\n",
    "# def random_string(stringLength=10):\n",
    "#     \"\"\"Generate a random string of fixed length \"\"\"\n",
    "#     return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "\n",
    "# print(\"Products between {} and {}\".format(1, 75000000))\n",
    "# product_ids = [x for x in range(1, 75000000)]\n",
    "# dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "#          '2020-07-09', '2020-07-10']\n",
    "# seller_ids = [x for x in range(1, 10)]\n",
    "\n",
    "\n",
    "# #   Generate products\n",
    "# products = [[0, \"product_0\", 22]]\n",
    "# for p in tqdm(product_ids):\n",
    "#     products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\n",
    "# #   Save dataframe\n",
    "# df = pd.DataFrame(products)\n",
    "# df.columns = [\"product_id\", \"product_name\", \"price\"]\n",
    "# df.to_csv(\"products.csv\", index=False)\n",
    "# del df\n",
    "# del products\n",
    "\n",
    "# #   Generate sellers\n",
    "# sellers = [[0, \"seller_0\", 2500000]]\n",
    "# for s in tqdm(seller_ids):\n",
    "#     sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "# #   Save dataframe\n",
    "# df = pd.DataFrame(sellers)\n",
    "# df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "# df.to_csv(\"sellers.csv\", index=False)\n",
    "\n",
    "# #   Generate sales\n",
    "# total_rows = 500000\n",
    "# prod_zero = int(total_rows * 0.95)\n",
    "# prod_others = total_rows - prod_zero + 1\n",
    "# df_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\n",
    "# with open('sales.csv', 'w', newline='') as f:\n",
    "#     csvwriter = csv.writer(f)\n",
    "#     csvwriter.writerows(df_array)\n",
    "\n",
    "# order_id = 0\n",
    "# for i in tqdm(range(0, 40)):\n",
    "#     df_array = []\n",
    "\n",
    "#     for i in range(0, prod_zero):\n",
    "#         order_id += 1\n",
    "#         df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\n",
    "\n",
    "#     with open('sales.csv', 'a', newline='') as f:\n",
    "#         csvwriter = csv.writer(f)\n",
    "#         csvwriter.writerows(df_array)\n",
    "\n",
    "#     df_array = []\n",
    "#     for i in range(0, prod_others):\n",
    "#         order_id += 1\n",
    "#         df_array.append(\n",
    "#             [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\n",
    "#              random.randint(1, 100), random_string(500)])\n",
    "\n",
    "#     with open('sales.csv', 'a', newline='') as f:\n",
    "#         csvwriter = csv.writer(f)\n",
    "#         csvwriter.writerows(df_array)\n",
    "\n",
    "# print(\"Done\")\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "#     .appName(\"Exercise1\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# products = spark.read.csv(\n",
    "#     \"products.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# products.show()\n",
    "# products.write.parquet(\"products_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# sales = spark.read.csv(\n",
    "#     \"sales.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# sales.show()\n",
    "# sales.repartition(200, col(\"product_id\")).write.parquet(\"sales_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# sellers = spark.read.csv(\n",
    "#     \"sellers.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# sellers.show()\n",
    "# sellers.write.parquet(\"sellers_parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a1b2725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://nortons-mbp.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa8581eb160>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c21fa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "#     .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "#     .appName(\"Exercise1\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bba0cd",
   "metadata": {},
   "source": [
    "#### Warm-Up #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304f44c",
   "metadata": {},
   "source": [
    "- Find out how many orders, how many products and how many sellers are in the data.\n",
    "- How many products have been sold at least once? Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4395dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7cb66e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet\n",
    "product_table = spark.read.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51301332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from cdv\n",
    "products_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3320315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83880ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./sellers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "660f5188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Orders: 20000040\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of orders\n",
    "print(\"Number of Orders: {}\".format(sales_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0b8a566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sellers: 10\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of sellers\n",
    "print(\"Number of sellers: {}\".format(sellers_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a22b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 75000000\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of products\n",
    "print(\"Number of products: {}\".format(products_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b42c4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n",
      "+--------------------------------+\n",
      "|products_sold_at_least_one_count|\n",
      "+--------------------------------+\n",
      "|                          993299|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output how many products have been actually sold at least once\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_table.agg(countDistinct(col(\"product_id\")).alias(\"products_sold_at_least_one_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee1bdc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "+----------+--------+\n",
      "|product_id|     cnt|\n",
      "+----------+--------+\n",
      "|         0|19000000|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\"))\\\n",
    "            .agg(count(\"*\").alias(\"cnt\"))\\\n",
    "                 .orderBy(col(\"cnt\").desc())\\\n",
    "                 .limit(1)\\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cebbe9",
   "metadata": {},
   "source": [
    "#### Warm-up #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd0ed0",
   "metadata": {},
   "source": [
    "How many distinct products have been sold in each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "86b00257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      date|distinct_products_sold|\n",
      "+----------+----------------------+\n",
      "|2020-07-04|                100294|\n",
      "|2020-07-03|                100224|\n",
      "|2020-07-10|                100218|\n",
      "|2020-07-08|                100048|\n",
      "|2020-07-05|                 99991|\n",
      "|2020-07-06|                 99869|\n",
      "|2020-07-09|                 99801|\n",
      "|2020-07-02|                 99768|\n",
      "|2020-07-01|                 99755|\n",
      "|2020-07-07|                 99453|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.groupby(col(\"date\"))\\\n",
    "            .agg(countDistinct(col(\"product_id\"))\\\n",
    "                 .alias(\"distinct_products_sold\"))\\\n",
    "                    .orderBy(col(\"distinct_products_sold\").desc())\\\n",
    "                        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6178af",
   "metadata": {},
   "source": [
    "#### Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833126ff",
   "metadata": {},
   "source": [
    "##### What is the average revenue of the orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41ab8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "# revenue = price * quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "003be732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1245.9236386027228|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do the join the print the results\n",
    "sales_table.join(products_table, sales_table[\"product_id\"]==products_table[\"product_id\"], \"inner\")\\\n",
    "                     .agg(avg(products_table[\"price\"]*sales_table[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334647a7",
   "metadata": {},
   "source": [
    "- The important thing to observe here is that we are NOT salting ALL the products, but only those that drive skewness (in the example we are getting the 100 most frequent products). \n",
    "- Salting the whole dataset would be problematic since the number of rows would grow linearly on the “salting factor”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3bb50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Check and select the skewed keys \n",
    "# In this case we are retrieving the top 100 keys: these will be the only salted keys.\n",
    "results = sales_table.groupby(sales_table[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "432995ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0935986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate the salted key\n",
    "products_table = products_table.join(broadcast(replicated_df),\n",
    "                                     products_table[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products_table[\"product_id\"]).otherwise(\n",
    "    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "sales_table = sales_table.withColumn(\"salted_join_key\", when(sales_table[\"product_id\"].isin(replicated_products),\n",
    "                                                             concat(sales_table[\"product_id\"], lit(\"-\"),\n",
    "                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n",
    "                                                                        IntegerType()))).otherwise(\n",
    "    sales_table[\"product_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ee145553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1245.9236386027228|\n",
      "+------------------------------+\n",
      "\n",
      "None\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "#   Step 4: Finally let's do the join\n",
    "print(sales_table.join(products_table, sales_table[\"salted_join_key\"] == products_table[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acfde88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5971ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
