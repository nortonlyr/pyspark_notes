{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea0eebe",
   "metadata": {},
   "source": [
    "reference https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4da3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products between 1 and 75000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74999999/74999999 [05:43<00:00, 218288.10it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 7917.10it/s]\n",
      "100%|██████████| 40/40 [1:00:59<00:00, 91.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "|         1|   product_1|   30|\n",
      "|         2|   product_2|   91|\n",
      "|         3|   product_3|   37|\n",
      "|         4|   product_4|  145|\n",
      "|         5|   product_5|  128|\n",
      "|         6|   product_6|   66|\n",
      "|         7|   product_7|  145|\n",
      "|         8|   product_8|   51|\n",
      "|         9|   product_9|   44|\n",
      "|        10|  product_10|   53|\n",
      "|        11|  product_11|   13|\n",
      "|        12|  product_12|  104|\n",
      "|        13|  product_13|  102|\n",
      "|        14|  product_14|   24|\n",
      "|        15|  product_15|   14|\n",
      "|        16|  product_16|   38|\n",
      "|        17|  product_17|   72|\n",
      "|        18|  product_18|   16|\n",
      "|        19|  product_19|   46|\n",
      "+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-03|             98|frlnwjcoaxsaubnat...|\n",
      "|       2|         0|        0|2020-07-07|             23|zsnrbwrlflvqqmbcz...|\n",
      "|       3|         0|        0|2020-07-02|             79|gmxnirkafafnohboh...|\n",
      "|       4|         0|        0|2020-07-07|              5|xrgknaskXkfcxcnzj...|\n",
      "|       5|         0|        0|2020-07-10|             79|tzkqoynsqnfomkpbt...|\n",
      "|       6|         0|        0|2020-07-05|             87|qoluiczrckaygkzbi...|\n",
      "|       7|         0|        0|2020-07-08|             14|ivwpwrpuhrjgjdauj...|\n",
      "|       8|         0|        0|2020-07-02|             64|hoalxshwHpqgyvqtm...|\n",
      "|       9|         0|        0|2020-07-02|             45|vysrvsdfvekabcmwo...|\n",
      "|      10|         0|        0|2020-07-05|             16|poiemeiqharpjqkao...|\n",
      "|      11|         0|        0|2020-07-09|              4|badjqluozzjHbbjkv...|\n",
      "|      12|         0|        0|2020-07-02|             58|fdgikecrmegaxfpvO...|\n",
      "|      13|         0|        0|2020-07-02|             56|zhrkbicjlasuqqwsl...|\n",
      "|      14|         0|        0|2020-07-04|             43|sivmclqcgiaspgomj...|\n",
      "|      15|         0|        0|2020-07-05|             39|usobmyZrxjdzdrecl...|\n",
      "|      16|         0|        0|2020-07-04|             79|zxbixfkhmydtewfje...|\n",
      "|      17|         0|        0|2020-07-10|             81|aancisgpjaueusynm...|\n",
      "|      18|         0|        0|2020-07-07|             62|gwkkxzjpdaaaskune...|\n",
      "|      19|         0|        0|2020-07-07|             13|jmltpvkcizhepIwwh...|\n",
      "|      20|         0|        0|2020-07-03|             69|sgicvswximmsqqtuj...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|     1375559|\n",
      "|        2|   seller_2|      205349|\n",
      "|        3|   seller_3|       71546|\n",
      "|        4|   seller_4|     1315668|\n",
      "|        5|   seller_5|      627802|\n",
      "|        6|   seller_6|     1997104|\n",
      "|        7|   seller_7|      593329|\n",
      "|        8|   seller_8|       24388|\n",
      "|        9|   seller_9|      348255|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download from source\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import csv\n",
    "# import random\n",
    "# import string\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# random.seed(1999)\n",
    "\n",
    "# letters = string.ascii_lowercase\n",
    "# letters_upper = string.ascii_uppercase\n",
    "# for _i in range(0, 10):\n",
    "#     letters += letters\n",
    "\n",
    "# for _i in range(0, 10):\n",
    "#     letters += letters_upper\n",
    "\n",
    "\n",
    "# def random_string(stringLength=10):\n",
    "#     \"\"\"Generate a random string of fixed length \"\"\"\n",
    "#     return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "\n",
    "# print(\"Products between {} and {}\".format(1, 75000000))\n",
    "# product_ids = [x for x in range(1, 75000000)]\n",
    "# dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "#          '2020-07-09', '2020-07-10']\n",
    "# seller_ids = [x for x in range(1, 10)]\n",
    "\n",
    "\n",
    "# #   Generate products\n",
    "# products = [[0, \"product_0\", 22]]\n",
    "# for p in tqdm(product_ids):\n",
    "#     products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\n",
    "# #   Save dataframe\n",
    "# df = pd.DataFrame(products)\n",
    "# df.columns = [\"product_id\", \"product_name\", \"price\"]\n",
    "# df.to_csv(\"products.csv\", index=False)\n",
    "# del df\n",
    "# del products\n",
    "\n",
    "# #   Generate sellers\n",
    "# sellers = [[0, \"seller_0\", 2500000]]\n",
    "# for s in tqdm(seller_ids):\n",
    "#     sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "# #   Save dataframe\n",
    "# df = pd.DataFrame(sellers)\n",
    "# df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "# df.to_csv(\"sellers.csv\", index=False)\n",
    "\n",
    "# #   Generate sales\n",
    "# total_rows = 500000\n",
    "# prod_zero = int(total_rows * 0.95)\n",
    "# prod_others = total_rows - prod_zero + 1\n",
    "# df_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\n",
    "# with open('sales.csv', 'w', newline='') as f:\n",
    "#     csvwriter = csv.writer(f)\n",
    "#     csvwriter.writerows(df_array)\n",
    "\n",
    "# order_id = 0\n",
    "# for i in tqdm(range(0, 40)):\n",
    "#     df_array = []\n",
    "\n",
    "#     for i in range(0, prod_zero):\n",
    "#         order_id += 1\n",
    "#         df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\n",
    "\n",
    "#     with open('sales.csv', 'a', newline='') as f:\n",
    "#         csvwriter = csv.writer(f)\n",
    "#         csvwriter.writerows(df_array)\n",
    "\n",
    "#     df_array = []\n",
    "#     for i in range(0, prod_others):\n",
    "#         order_id += 1\n",
    "#         df_array.append(\n",
    "#             [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\n",
    "#              random.randint(1, 100), random_string(500)])\n",
    "\n",
    "#     with open('sales.csv', 'a', newline='') as f:\n",
    "#         csvwriter = csv.writer(f)\n",
    "#         csvwriter.writerows(df_array)\n",
    "\n",
    "# print(\"Done\")\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "#     .appName(\"Exercise1\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# products = spark.read.csv(\n",
    "#     \"products.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# products.show()\n",
    "# products.write.parquet(\"products_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# sales = spark.read.csv(\n",
    "#     \"sales.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# sales.show()\n",
    "# sales.repartition(200, col(\"product_id\")).write.parquet(\"sales_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# sellers = spark.read.csv(\n",
    "#     \"sellers.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# sellers.show()\n",
    "# sellers.write.parquet(\"sellers_parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f2d0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://nortons-mbp.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa8581eb160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21fa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "#     .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "#     .appName(\"Exercise1\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb668c",
   "metadata": {},
   "source": [
    "#### Warm-Up #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07852886",
   "metadata": {},
   "source": [
    "- Find out how many orders, how many products and how many sellers are in the data.\n",
    "- How many products have been sold at least once? Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a6895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet\n",
    "product_table = spark.read.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f973845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from cdv\n",
    "products_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ae2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a44a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./sellers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2bd66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Orders: 20000040\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of orders\n",
    "print(\"Number of Orders: {}\".format(sales_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebe80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sellers: 10\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of sellers\n",
    "print(\"Number of sellers: {}\".format(sellers_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "317e226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 75000000\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of products\n",
    "print(\"Number of products: {}\".format(products_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c81a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n",
      "+--------------------------------+\n",
      "|products_sold_at_least_one_count|\n",
      "+--------------------------------+\n",
      "|                          993299|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output how many products have been actually sold at least once\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_table.agg(countDistinct(col(\"product_id\")).alias(\"products_sold_at_least_one_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2600d2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "+----------+--------+\n",
      "|product_id|     cnt|\n",
      "+----------+--------+\n",
      "|         0|19000000|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\"))\\\n",
    "            .agg(count(\"*\").alias(\"cnt\"))\\\n",
    "                 .orderBy(col(\"cnt\").desc())\\\n",
    "                 .limit(1)\\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3365ab2",
   "metadata": {},
   "source": [
    "#### Warm-up #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2c6d7",
   "metadata": {},
   "source": [
    "How many distinct products have been sold in each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55754cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      date|distinct_products_sold|\n",
      "+----------+----------------------+\n",
      "|2020-07-04|                100294|\n",
      "|2020-07-03|                100224|\n",
      "|2020-07-10|                100218|\n",
      "|2020-07-08|                100048|\n",
      "|2020-07-05|                 99991|\n",
      "|2020-07-06|                 99869|\n",
      "|2020-07-09|                 99801|\n",
      "|2020-07-02|                 99768|\n",
      "|2020-07-01|                 99755|\n",
      "|2020-07-07|                 99453|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.groupby(col(\"date\"))\\\n",
    "            .agg(countDistinct(col(\"product_id\"))\\\n",
    "                 .alias(\"distinct_products_sold\"))\\\n",
    "                    .orderBy(col(\"distinct_products_sold\").desc())\\\n",
    "                        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50860258",
   "metadata": {},
   "source": [
    "#### Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb05596",
   "metadata": {},
   "source": [
    "##### What is the average revenue of the orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b96f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "# revenue = price * quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd974a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the join the print the results\n",
    "sales_table.join(products_table, sales_table[\"product_id\"]==products_table[\"product_id\"], \"inner\")\\\n",
    "                     .agg(avg(products_table[\"price\"]*sales_table[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c5180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6893878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f0fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
