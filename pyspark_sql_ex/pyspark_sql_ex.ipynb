{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9263f0",
   "metadata": {},
   "source": [
    "reference https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b4da3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from source\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import csv\n",
    "# import random\n",
    "# import string\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# random.seed(1999)\n",
    "\n",
    "# letters = string.ascii_lowercase\n",
    "# letters_upper = string.ascii_uppercase\n",
    "# for _i in range(0, 10):\n",
    "#     letters += letters\n",
    "\n",
    "# for _i in range(0, 10):\n",
    "#     letters += letters_upper\n",
    "\n",
    "\n",
    "# def random_string(stringLength=10):\n",
    "#     \"\"\"Generate a random string of fixed length \"\"\"\n",
    "#     return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "\n",
    "# print(\"Products between {} and {}\".format(1, 75000000))\n",
    "# product_ids = [x for x in range(1, 75000000)]\n",
    "# dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "#          '2020-07-09', '2020-07-10']\n",
    "# seller_ids = [x for x in range(1, 10)]\n",
    "\n",
    "\n",
    "# #   Generate products\n",
    "# products = [[0, \"product_0\", 22]]\n",
    "# for p in tqdm(product_ids):\n",
    "#     products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\n",
    "# #   Save dataframe\n",
    "# df = pd.DataFrame(products)\n",
    "# df.columns = [\"product_id\", \"product_name\", \"price\"]\n",
    "# df.to_csv(\"products.csv\", index=False)\n",
    "# del df\n",
    "# del products\n",
    "\n",
    "# #   Generate sellers\n",
    "# sellers = [[0, \"seller_0\", 2500000]]\n",
    "# for s in tqdm(seller_ids):\n",
    "#     sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "# #   Save dataframe\n",
    "# df = pd.DataFrame(sellers)\n",
    "# df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "# df.to_csv(\"sellers.csv\", index=False)\n",
    "\n",
    "# #   Generate sales\n",
    "# total_rows = 500000\n",
    "# prod_zero = int(total_rows * 0.95)\n",
    "# prod_others = total_rows - prod_zero + 1\n",
    "# df_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\n",
    "# with open('sales.csv', 'w', newline='') as f:\n",
    "#     csvwriter = csv.writer(f)\n",
    "#     csvwriter.writerows(df_array)\n",
    "\n",
    "# order_id = 0\n",
    "# for i in tqdm(range(0, 40)):\n",
    "#     df_array = []\n",
    "\n",
    "#     for i in range(0, prod_zero):\n",
    "#         order_id += 1\n",
    "#         df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\n",
    "\n",
    "#     with open('sales.csv', 'a', newline='') as f:\n",
    "#         csvwriter = csv.writer(f)\n",
    "#         csvwriter.writerows(df_array)\n",
    "\n",
    "#     df_array = []\n",
    "#     for i in range(0, prod_others):\n",
    "#         order_id += 1\n",
    "#         df_array.append(\n",
    "#             [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\n",
    "#              random.randint(1, 100), random_string(500)])\n",
    "\n",
    "#     with open('sales.csv', 'a', newline='') as f:\n",
    "#         csvwriter = csv.writer(f)\n",
    "#         csvwriter.writerows(df_array)\n",
    "\n",
    "# print(\"Done\")\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "#     .appName(\"Exercise1\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# products = spark.read.csv(\n",
    "#     \"products.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# products.show()\n",
    "# products.write.parquet(\"products_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# sales = spark.read.csv(\n",
    "#     \"sales.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# sales.show()\n",
    "# sales.repartition(200, col(\"product_id\")).write.parquet(\"sales_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# sellers = spark.read.csv(\n",
    "#     \"sellers.csv\", header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "# sellers.show()\n",
    "# sellers.write.parquet(\"sellers_parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c0cd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://nortons-mbp.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa8581eb160>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c21fa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "#     .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "#     .appName(\"Exercise1\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5928f",
   "metadata": {},
   "source": [
    "#### Warm-Up #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e2566",
   "metadata": {},
   "source": [
    "- Find out how many orders, how many products and how many sellers are in the data.\n",
    "- How many products have been sold at least once? Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ade60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0dfdc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet\n",
    "product_table = spark.read.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "819f7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from cdv\n",
    "products_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "92a785ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d7d5c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_table = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .load('./sellers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "da54eb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Orders: 20000040\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of orders\n",
    "print(\"Number of Orders: {}\".format(sales_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dbb2f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sellers: 10\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of sellers\n",
    "print(\"Number of sellers: {}\".format(sellers_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ca15fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 75000000\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of products\n",
    "print(\"Number of products: {}\".format(products_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a5f04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n",
      "+--------------------------------+\n",
      "|products_sold_at_least_one_count|\n",
      "+--------------------------------+\n",
      "|                          993299|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output how many products have been actually sold at least once\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_table.agg(countDistinct(col(\"product_id\")).alias(\"products_sold_at_least_one_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "23dc7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "+----------+--------+\n",
      "|product_id|     cnt|\n",
      "+----------+--------+\n",
      "|         0|19000000|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\"))\\\n",
    "            .agg(count(\"*\").alias(\"cnt\"))\\\n",
    "                 .orderBy(col(\"cnt\").desc())\\\n",
    "                 .limit(1)\\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f030341",
   "metadata": {},
   "source": [
    "#### Warm-up #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a220652",
   "metadata": {},
   "source": [
    "How many distinct products have been sold in each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "93e2b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      date|distinct_products_sold|\n",
      "+----------+----------------------+\n",
      "|2020-07-04|                100294|\n",
      "|2020-07-03|                100224|\n",
      "|2020-07-10|                100218|\n",
      "|2020-07-08|                100048|\n",
      "|2020-07-05|                 99991|\n",
      "|2020-07-06|                 99869|\n",
      "|2020-07-09|                 99801|\n",
      "|2020-07-02|                 99768|\n",
      "|2020-07-01|                 99755|\n",
      "|2020-07-07|                 99453|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.groupby(col(\"date\"))\\\n",
    "            .agg(countDistinct(col(\"product_id\"))\\\n",
    "                 .alias(\"distinct_products_sold\"))\\\n",
    "                    .orderBy(col(\"distinct_products_sold\").desc())\\\n",
    "                        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda67e7",
   "metadata": {},
   "source": [
    "#### Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156953b",
   "metadata": {},
   "source": [
    "##### What is the average revenue of the orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c21f7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "# revenue = price * quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7fa056c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1245.9236386027228|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do the join the print the results\n",
    "sales_table.join(products_table, sales_table[\"product_id\"]==products_table[\"product_id\"], \"inner\")\\\n",
    "                     .agg(avg(products_table[\"price\"]*sales_table[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f83b9",
   "metadata": {},
   "source": [
    "- The important thing to observe here is that we are NOT salting ALL the products, but only those that drive skewness (in the example we are getting the 100 most frequent products). \n",
    "- Salting the whole dataset would be problematic since the number of rows would grow linearly on the “salting factor”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d1fc2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Check and select the skewed keys \n",
    "# In this case we are retrieving the top 100 keys: these will be the only salted keys.\n",
    "results = sales_table.groupby(sales_table[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "572eec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4a679ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate the salted key\n",
    "products_table = products_table.join(broadcast(replicated_df),\n",
    "                                     products_table[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products_table[\"product_id\"]).otherwise(\n",
    "    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "sales_table = sales_table.withColumn(\"salted_join_key\", when(sales_table[\"product_id\"].isin(replicated_products),\n",
    "                                                             concat(sales_table[\"product_id\"], lit(\"-\"),\n",
    "                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n",
    "                                                                        IntegerType()))).otherwise(\n",
    "    sales_table[\"product_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b577642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1245.9236386027228|\n",
      "+------------------------------+\n",
      "\n",
      "None\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "#   Step 4: Finally let's do the join\n",
    "print(sales_table.join(products_table, sales_table[\"salted_join_key\"] == products_table[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb1e98",
   "metadata": {},
   "source": [
    "#### Exercise #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583abaa",
   "metadata": {},
   "source": [
    "Question number two was: “for each seller, what is the average % contribution of an order to the sellers’ daily quota?”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a9e5d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        7|8.510553537464308E-5|\n",
      "|        3|7.060842894390345E-4|\n",
      "|        8|0.002071646546208688|\n",
      "|        0|2.019736225290656E-5|\n",
      "|        5|8.038980497173718E-5|\n",
      "|        6|2.534518215186283...|\n",
      "|        9|1.449276275189591...|\n",
      "|        1|3.670188787905770...|\n",
      "|        4|3.845384604576876E-5|\n",
      "|        2| 2.45672194595152E-4|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sales_table.join(sellers_table, sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\")\\\n",
    "#             .withColumn(\"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_terget\"]\\\n",
    "#             .groupBy(sales_table[\"seller_id\"])\\\n",
    "#             .agg(avg(\"ratio\")).show())\n",
    "\n",
    "sales_table.join(sellers_table, sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n",
    ").groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show()\n",
    "\n",
    "# Wrong way to do this - Skewed\n",
    "# (Note that Spark will probably broadcast the table anyway, unless we forbid it throug the configuration paramters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6a6d1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        7|8.510553537463727E-5|\n",
      "|        3|7.060842894390333E-4|\n",
      "|        8|0.002071646546208...|\n",
      "|        0|2.019736225262764E-5|\n",
      "|        5|8.038980497175842E-5|\n",
      "|        6|2.534518215186317...|\n",
      "|        9|1.449276275189586...|\n",
      "|        1|3.670188787904589E-5|\n",
      "|        4|3.845384604576359E-5|\n",
      "|        2|2.456721945951489E-4|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correct way through broarcasting\n",
    "sales_table.join(broadcast(sellers_table), sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n",
    ").groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0a31b",
   "metadata": {},
   "source": [
    "#### Exercise #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ff301",
   "metadata": {},
   "source": [
    "Question: “Who are the second most selling and the least selling persons (sellers) for each product? Who are those for the product with product_id = 0”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "19774075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of pieces sold by each seller for each product and sort by the pieces sold\n",
    "sales_table = sales_table.groupby(col(\"product_id\"), col(\"seller_id\"))\\\n",
    "                            .agg(sum(\"num_pieces_sold\").alias(\"num_pieces_sold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0eafe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the window functions, one will sort ascending the other one descending. Partition by the product_id\n",
    "# and sort by pieces sold\n",
    "from pyspark.sql import Row, Window\n",
    "window_desc = Window.partitionBy(col(\"product_id\")).orderBy(col('num_pieces_sold').desc())\n",
    "window_asc = Window.partitionBy(col(\"product_id\")).orderBy(col('num_pieces_sold').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f2bf2467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x7fa89a0f49a0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6fbb6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dense Rank (to avoid holes)\n",
    "sales_table = sales_table.withColumn(\"rank_asc\", dense_rank().over(window_asc))\\\n",
    "                            .withColumn(\"rank_desc\", dense_rank().over(window_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c42f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+--------+---------+\n",
      "|product_id|seller_id|num_pieces_sold|rank_asc|rank_desc|\n",
      "+----------+---------+---------------+--------+---------+\n",
      "|  10005071|        7|           76.0|       1|        1|\n",
      "|  10022361|        6|           92.0|       1|        1|\n",
      "|  10027928|        4|           87.0|       1|        1|\n",
      "|  10038026|        1|           52.0|       1|        1|\n",
      "|  10106159|        6|          100.0|       1|        1|\n",
      "|  10117814|        5|           65.0|       1|        1|\n",
      "|  10124701|        3|           97.0|       1|        1|\n",
      "|  10124876|        2|           72.0|       1|        1|\n",
      "|  10125598|        4|           29.0|       1|        1|\n",
      "|  10137910|        6|           68.0|       1|        1|\n",
      "|   1014157|        7|           70.0|       1|        1|\n",
      "|  10143170|        6|           55.0|       1|        1|\n",
      "|  10145275|        8|           94.0|       1|        1|\n",
      "|  10162724|        4|           86.0|       1|        1|\n",
      "|  10179037|        4|           75.0|       1|        1|\n",
      "|    102113|        8|           39.0|       1|        1|\n",
      "|  10214169|        4|           16.0|       1|        1|\n",
      "|  10272594|        2|           81.0|       1|        1|\n",
      "|  10283350|        1|           42.0|       1|        1|\n",
      "|   1033297|        6|           38.0|       1|        1|\n",
      "+----------+---------+---------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f54a488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get products that only have one row OR the products in which multiple sellers sold the same amount\n",
    "# (i.e. all the employees that ever sold the product, sold the same exact amount)\n",
    "second_seller = sales_table.where(col(\"rank_desc\") == 2).select(\n",
    "    col(\"product_id\").alias(\"second_seller_product_id\"), col(\"seller_id\").alias(\"second_seller_seller_id\"),\n",
    "    lit(\"Second top seller\").alias(\"type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aec9f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------+--------------------+\n",
      "|single_seller_product_id|single_seller_id|                type|\n",
      "+------------------------+----------------+--------------------+\n",
      "|                11627820|               5|Only seller or mu...|\n",
      "|                15234937|               6|Only seller or mu...|\n",
      "|                17794988|               6|Only seller or mu...|\n",
      "|                 2020468|               8|Only seller or mu...|\n",
      "|                20922486|               3|Only seller or mu...|\n",
      "|                23472480|               2|Only seller or mu...|\n",
      "|                24729156|               1|Only seller or mu...|\n",
      "|                26261709|               2|Only seller or mu...|\n",
      "|                29470571|               2|Only seller or mu...|\n",
      "|                 3134676|               3|Only seller or mu...|\n",
      "|                33178488|               3|Only seller or mu...|\n",
      "|                35465703|               3|Only seller or mu...|\n",
      "|                36078484|               2|Only seller or mu...|\n",
      "|                36595909|               8|Only seller or mu...|\n",
      "|                38243706|               7|Only seller or mu...|\n",
      "|                41580020|               2|Only seller or mu...|\n",
      "|                41788463|               7|Only seller or mu...|\n",
      "|                42770361|               4|Only seller or mu...|\n",
      "|                44356182|               8|Only seller or mu...|\n",
      "|                44396874|               1|Only seller or mu...|\n",
      "+------------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_seller.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4bd7c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the second top sellers\n",
    "second_seller = sales_table.where(col(\"rank_desc\")==2).select(\n",
    "                col(\"product_id\").alias(\"second_seller_product_id\"), col(\"seller_id\").alias(\n",
    "                    \"second_seller_sell_id\"), \n",
    "                    lit(\"Second top seller\").alias(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c285750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------+-----------------+\n",
      "|second_seller_product_id|second_seller_sell_id|             type|\n",
      "+------------------------+---------------------+-----------------+\n",
      "|                11627820|                    5|Second top seller|\n",
      "|                15234937|                    6|Second top seller|\n",
      "|                17794988|                    6|Second top seller|\n",
      "|                 2020468|                    8|Second top seller|\n",
      "|                20922486|                    3|Second top seller|\n",
      "|                23472480|                    2|Second top seller|\n",
      "|                24729156|                    1|Second top seller|\n",
      "|                26261709|                    2|Second top seller|\n",
      "|                29470571|                    2|Second top seller|\n",
      "|                 3134676|                    3|Second top seller|\n",
      "|                33178488|                    3|Second top seller|\n",
      "|                35465703|                    3|Second top seller|\n",
      "|                36078484|                    2|Second top seller|\n",
      "|                36595909|                    8|Second top seller|\n",
      "|                38243706|                    7|Second top seller|\n",
      "|                41580020|                    2|Second top seller|\n",
      "|                41788463|                    7|Second top seller|\n",
      "|                42770361|                    4|Second top seller|\n",
      "|                44356182|                    8|Second top seller|\n",
      "|                44396874|                    1|Second top seller|\n",
      "+------------------------+---------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_seller.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the least sellers and exclude those rows that are already included in the first piece\n",
    "# We also exclude the \"second top sellers\" that are also \"least sellers\"\n",
    "least_seller = sales_table.where(col(\"rank_asc\") == 1).select(\n",
    "    col(\"product_id\"), col(\"seller_id\"),\n",
    "    lit(\"Least Seller\").alias(\"type\")\n",
    ").join(single_seller, (sales_table[\"seller_id\"] == single_seller[\"single_seller_seller_id\"]) & (\n",
    "        sales_table[\"product_id\"] == single_seller[\"single_seller_product_id\"]), \"left_anti\"). \\\n",
    "    join(second_seller, (sales_table[\"seller_id\"] == second_seller[\"second_seller_seller_id\"]) & (\n",
    "        sales_table[\"product_id\"] == second_seller[\"second_seller_product_id\"]), \"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union all the pieces\n",
    "union_table = least_seller.select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"seller_id\"),\n",
    "        col(\"tyoe\")\n",
    ").union(second_seller.select(\n",
    "        col(\"second_seller_product_id\").alias(\"product_id\"),\n",
    "        col(\"seconc_seller_seller_id\"),\n",
    "        col(\"type\")\n",
    ")).union(single_seller.select(\n",
    "        col(\"single_seller_product_id\"),\n",
    "        col(\"single_seller_seller_id\").alias(\"seller_id\"),\n",
    "        col(\"type\")\n",
    "))\n",
    "union_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba88ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fc100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
